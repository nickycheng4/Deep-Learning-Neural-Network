{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Softmax(object):\n",
    "\n",
    "  def __init__(self, dims=[10, 3073]):\n",
    "    self.init_weights(dims=dims)\n",
    "\n",
    "  def init_weights(self, dims):\n",
    "    \"\"\"\n",
    "\tInitializes the weight matrix of the Softmax classifier.  \n",
    "\tNote that it has shape (C, D) where C is the number of \n",
    "\tclasses and D is the feature size.\n",
    "\t\"\"\"\n",
    "    self.W = np.random.normal(size=dims) * 0.0001\n",
    "\n",
    "  def loss(self, X, y):\n",
    "    \"\"\"\n",
    "    Calculates the softmax loss.\n",
    "  \n",
    "    Inputs have dimension D, there are C classes, and we operate on minibatches\n",
    "    of N examples.\n",
    "  \n",
    "    Inputs:\n",
    "    - X: A numpy array of shape (N, D) containing a minibatch of data.\n",
    "    - y: A numpy array of shape (N,) containing training labels; y[i] = c means\n",
    "      that X[i] has label c, where 0 <= c < C.\n",
    "  \n",
    "    Returns a tuple of:\n",
    "    - loss as single float\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the loss to zero.\n",
    "    loss = 0.0\n",
    "\n",
    "    # ================================================================ #\n",
    "    # YOUR CODE HERE:\n",
    "\t#   Calculate the normalized softmax loss.  Store it as the variable loss.\n",
    "    #   (That is, calculate the sum of the losses of all the training \n",
    "    #   set margins, and then normalize the loss by the number of \n",
    "\t#\ttraining examples.)\n",
    "    # ================================================================ #\n",
    "    scores = np.dot(self.W, X.T)\n",
    "\n",
    "    for i in range(scores.shape[1]):\n",
    "      temp = scores[:, i]\n",
    "      #print(temp)\n",
    "      temp -= np.max(temp)\n",
    "      #print(temp)\n",
    "      cur_class_score = temp[y[i]]\n",
    "      log_v = np.sum(np.exp(temp))\n",
    "      loss = loss + np.log(log_v)\n",
    "      loss = loss - cur_class_score\n",
    "      #print(loss)\n",
    "    loss = loss / X.shape[0]\n",
    "    \n",
    "    # ================================================================ #\n",
    "    # END YOUR CODE HERE\n",
    "    # ================================================================ #\n",
    "\n",
    "    return loss\n",
    "\n",
    "  def loss_and_grad(self, X, y):\n",
    "    \"\"\"\n",
    "\tSame as self.loss(X, y), except that it also returns the gradient.\n",
    "\n",
    "\tOutput: grad -- a matrix of the same dimensions as W containing \n",
    "\t\tthe gradient of the loss with respect to W.\n",
    "\t\"\"\"\n",
    "\n",
    "    # Initialize the loss and gradient to zero.\n",
    "    loss = 0.0\n",
    "    grad = np.zeros_like(self.W)\n",
    "    total_score = np.dot(self.W, X.T)\n",
    "\n",
    "    # ================================================================ #\n",
    "    # YOUR CODE HERE:\n",
    "\t#   Calculate the softmax loss and the gradient. Store the gradient\n",
    "\t#   as the variable grad.\n",
    "    # ================================================================ #\n",
    "    \n",
    "\n",
    "    train_data = X.shape[0]\n",
    "    class_data = self.W.shape[0]\n",
    "\n",
    "    for i in range(train_data):\n",
    "\n",
    "      cur_s = total_score[:, i]\n",
    "      cur_s -= np.max(cur_s)\n",
    "      temp_score = cur_s[y[i]]\n",
    "      sum_exp = np.sum(np.exp(cur_s))\n",
    "\n",
    "      loss += np.log(sum_exp)\n",
    "      loss -= temp_score\n",
    "\n",
    "      for j in range(class_data):\n",
    "        grad[j] += (np.exp(cur_s[j]) / sum_exp) * X[i]\n",
    "\n",
    "      grad[y[i]] -= X[i]\n",
    "    loss /= train_data\n",
    "    grad /= train_data\n",
    "    \n",
    "    # ================================================================ #\n",
    "    # END YOUR CODE HERE\n",
    "    # ================================================================ #\n",
    "\n",
    "    return loss, grad\n",
    "\n",
    "  def grad_check_sparse(self, X, y, your_grad, num_checks=10, h=1e-5):\n",
    "    \"\"\"\n",
    "    sample a few random elements and only return numerical\n",
    "    in these dimensions.\n",
    "    \"\"\"\n",
    "  \n",
    "    for i in np.arange(num_checks):\n",
    "      ix = tuple([np.random.randint(m) for m in self.W.shape])\n",
    "  \n",
    "      oldval = self.W[ix]\n",
    "      self.W[ix] = oldval + h # increment by h\n",
    "      fxph = self.loss(X, y)\n",
    "      self.W[ix] = oldval - h # decrement by h\n",
    "      fxmh = self.loss(X,y) # evaluate f(x - h)\n",
    "      self.W[ix] = oldval # reset\n",
    "  \n",
    "      grad_numerical = (fxph - fxmh) / (2 * h)\n",
    "      grad_analytic = your_grad[ix]\n",
    "      rel_error = abs(grad_numerical - grad_analytic) / (abs(grad_numerical) + abs(grad_analytic))\n",
    "      print('numerical: %f analytic: %f, relative error: %e' % (grad_numerical, grad_analytic, rel_error))\n",
    "\n",
    "  def fast_loss_and_grad(self, X, y):\n",
    "    \"\"\"\n",
    "    A vectorized implementation of loss_and_grad. It shares the same\n",
    "\tinputs and ouptuts as loss_and_grad.\n",
    "    \"\"\"\n",
    "    loss = 0.0\n",
    "    grad = np.zeros(self.W.shape) # initialize the gradient as zero\n",
    "  \n",
    "    # ================================================================ #\n",
    "    # YOUR CODE HERE:\n",
    "\t#   Calculate the softmax loss and gradient WITHOUT any for loops.\n",
    "    # ================================================================ #\n",
    "    train_count = X.shape[0]\n",
    "    total_score = np.dot(self.W, X.T)\n",
    "    total_score -= np.max(total_score, axis=0, keepdims=True)\n",
    "\n",
    "    exped_score = np.exp(total_score)\n",
    "    accu = exped_score / np.sum(exped_score, axis=0, keepdims=True)\n",
    "    act_prob = accu[y, range(train_count)]\n",
    "\n",
    "    logged_accu = -np.log(act_prob.clip(min=np.finfo(float).eps))\n",
    "    loss = np.sum(logged_accu) / train_count\n",
    "    #print(loss)\n",
    "    accu[y, range(train_count)] -= 1\n",
    "    grad = np.dot(accu, X)\n",
    "    grad /= train_count\n",
    "    \n",
    "    # ================================================================ #\n",
    "    # END YOUR CODE HERE\n",
    "    # ================================================================ #\n",
    "\n",
    "    return loss, grad\n",
    "\n",
    "  def train(self, X, y, learning_rate=1e-3, num_iters=100,\n",
    "            batch_size=200, verbose=False):\n",
    "    \"\"\"\n",
    "    Train this linear classifier using stochastic gradient descent.\n",
    "\n",
    "    Inputs:\n",
    "    - X: A numpy array of shape (N, D) containing training data; there are N\n",
    "      training samples each of dimension D.\n",
    "    - y: A numpy array of shape (N,) containing training labels; y[i] = c\n",
    "      means that X[i] has label 0 <= c < C for C classes.\n",
    "    - learning_rate: (float) learning rate for optimization.\n",
    "    - num_iters: (integer) number of steps to take when optimizing\n",
    "    - batch_size: (integer) number of training examples to use at each step.\n",
    "    - verbose: (boolean) If true, print progress during optimization.\n",
    "\n",
    "    Outputs:\n",
    "    A list containing the value of the loss function at each training iteration.\n",
    "    \"\"\"\n",
    "    num_train, dim = X.shape\n",
    "    num_classes = np.max(y) + 1 # assume y takes values 0...K-1 where K is number of classes\n",
    "\n",
    "    self.init_weights(dims=[np.max(y) + 1, X.shape[1]])\t# initializes the weights of self.W\n",
    "\n",
    "    # Run stochastic gradient descent to optimize W\n",
    "    loss_history = []\n",
    "\n",
    "    for it in np.arange(num_iters):\n",
    "      X_batch = None\n",
    "      y_batch = None\n",
    "\n",
    "      # ================================================================ #\n",
    "      # YOUR CODE HERE:\n",
    "      #   Sample batch_size elements from the training data for use in \n",
    "      #\t  gradient descent.  After sampling,\n",
    "      #     - X_batch should have shape: (dim, batch_size)\n",
    "\t  #     - y_batch should have shape: (batch_size,)\n",
    "\t  #   The indices should be randomly generated to reduce correlations\n",
    "\t  #   in the dataset.  Use np.random.choice.  It's okay to sample with\n",
    "\t  #   replacement.\n",
    "      # ================================================================ #\n",
    "      index = np.random.choice(X.shape[0], batch_size)\n",
    "\n",
    "      X_batch = X[index]\n",
    "      y_batch = y[index]\n",
    "      # ================================================================ #\n",
    "      # END YOUR CODE HERE\n",
    "      # ================================================================ #\n",
    "\n",
    "      # evaluate loss and gradient\n",
    "      loss, grad = self.fast_loss_and_grad(X_batch, y_batch)\n",
    "      loss_history.append(loss)\n",
    "\n",
    "      # ================================================================ #\n",
    "      # YOUR CODE HERE:\n",
    "      #   Update the parameters, self.W, with a gradient step \n",
    "      # ================================================================ #\n",
    "      self.W += -learning_rate * grad\n",
    "\n",
    "\t  # ================================================================ #\n",
    "      # END YOUR CODE HERE\n",
    "      # ================================================================ #\n",
    "\n",
    "      if verbose and it % 100 == 0:\n",
    "        print('iteration {} / {}: loss {}'.format(it, num_iters, loss))\n",
    "\n",
    "    return loss_history\n",
    "\n",
    "  def predict(self, X):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - X: N x D array of training data. Each row is a D-dimensional point.\n",
    "\n",
    "    Returns:\n",
    "    - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional\n",
    "      array of length N, and each element is an integer giving the predicted\n",
    "      class.\n",
    "    \"\"\"\n",
    "    y_pred = np.zeros(X.shape[1])\n",
    "    # ================================================================ #\n",
    "    # YOUR CODE HERE:\n",
    "    #   Predict the labels given the training data.\n",
    "    # ================================================================ #\n",
    "    total_score = np.dot(self.W, X.T)\n",
    "\n",
    "    y_pred = np.argmax(total_score, axis = 0)\n",
    "    # ================================================================ #\n",
    "    # END YOUR CODE HERE\n",
    "    # ================================================================ #\n",
    "\n",
    "    return y_pred\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
